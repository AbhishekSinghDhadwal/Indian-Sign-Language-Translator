{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelTraining-Squeezenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40EUHUDCDWvk"
      },
      "source": [
        "#Training Notebook - Indian Sign Language Translator\n",
        "Author : [Abhishek Singh Dhadwal](https://github.com/AbhishekSinghDhadwal)\n",
        "\n",
        "\n",
        "This notebook has been used to train the squeezenet model used in our project using Transfer Learning. Originally made for training via Colab, can be run locally by changing the required DIRs.\n",
        "\n",
        "If required, the code can be modified to transfer train via the following 4 models, you only need to download the pre-trained models and change the dir accordingly :\n",
        "\n",
        "1. SqueezeNet (original) - [weights](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5)\n",
        "2. DenseNet - [DenseNet-BC-121-32 weights](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5)\n",
        "3. ResNet 50 - [weights](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5)\n",
        "4. Inceptionv3 - [weights](https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDLYvp4qDWMd"
      },
      "source": [
        "### COLAB-ONLY SNIPPET\n",
        "The following snippet is used for mounting Colab to Google drive, do not run if you're using a local runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGJYPFjqQ7zv"
      },
      "source": [
        "from google.colab import drive\n",
        "import subprocess\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHFpNGQBI7cA"
      },
      "source": [
        "Importing libraries-\n",
        " do note that imageAI 2.1 is not compatible with tensorflow 2, hence we rollback to a previous version for tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NSwRpQ50YqA"
      },
      "source": [
        "# Downloading imageAI library\n",
        "!pip3 install imageai==2.1.5\n",
        "!pip3 install -q opencv-python\n",
        "!pip3 install -q pillow\n",
        "!pip install tensorflow-gpu==1.15.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fucJh2_scp"
      },
      "source": [
        "### Helper functions and required imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN0Ga7R2_Xnr"
      },
      "source": [
        "import os\n",
        "from imageai.Prediction.Custom import ModelTraining,CustomImagePrediction\n",
        "\n",
        "def input_model(model_name) :\n",
        "    model_trainer = ModelTraining()\n",
        "    if model_name == \"SqueezeNet\":\n",
        "        model_trainer.setModelTypeAsSqueezeNet()\n",
        "        modeldata = \"squeezenetdata\"\n",
        "    elif model_name == \"ResNet\":\n",
        "        model_trainer.setModelTypeAsResNet()\n",
        "        modeldata = \"resnetdata\"\n",
        "    elif model_name ==  \"Inceptionv3\":\n",
        "        model_trainer.setModelTypeAsInceptionV3()\n",
        "        modeldata = \"inceptiondata\"\n",
        "    elif model_name ==  \"DenseNet\":\n",
        "        model_trainer.setModelTypeAsDenseNet()\n",
        "        modeldata = \"densenetdata\"\n",
        "    else :\n",
        "        print(\"Invalid model name entered\")\n",
        "        exit(0)\n",
        "    return model_trainer,modeldata\n",
        "\n",
        "def getModelPredict(model_name, model_path, json_path, num_objects):\n",
        "    model = CustomImagePrediction()\n",
        "    if model_name == \"SqueezeNet\":\n",
        "        model.setModelTypeAsSqueezeNet()\n",
        "    elif model_name == \"ResNet\":\n",
        "        model.setModelTypeAsResNet()\n",
        "    elif model_name ==  \"Inceptionv3\":\n",
        "        model.setModelTypeAsInceptionV3()\n",
        "    elif model_name ==  \"DenseNet\":\n",
        "        model.setModelTypeAsDenseNet()\n",
        "    else :\n",
        "        print(\"Invalid model name entered\")\n",
        "        exit(0)\n",
        "    model.setModelPath(model_path)\n",
        "    model.setJsonPath(json_path)\n",
        "    model.loadFullModel(num_objects=num_objects)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHvEMZHi7ClX"
      },
      "source": [
        "## Inputs for pre-trained model weights and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe7euf8qyRIV"
      },
      "source": [
        "# ENTER YOUR DIRECTORIES AND DATA HERE - This data changes per test #########################################################################\n",
        "model,modeldata = input_model(\"SqueezeNet\") # Change as per requirements \n",
        "NUM_EPOCHS = 100\n",
        "model_rar_loc = \"path of pre-trained model compressed in rar format\"\n",
        "ds_rar_loc = \"path of dataset compressed in rar format\"\n",
        "dataset_name = \"the name of your parent dataset folder before extraction\" # Input for your dataset, should be in root Drive level\n",
        "model_data_dir = \"location where both trained models and json files should be stored\"\n",
        "num_classes = 10\n",
        "##############################################################################################################################################\n",
        "\n",
        "model_rar = os.path.basename(model_rar_loc)\n",
        "MODEL_H5 = model_rar.replace('.rar','') + \".h5\"\n",
        "dataset_rar = os.path.basename(ds_rar_loc)\n",
        "DATASET_NAME = dataset_rar.replace('.rar','')\n",
        "\n",
        "MODEL_DIR=model_data_dir+\"/models/\"\n",
        "!mkdir MODEL_DIR \n",
        "JSON_DIR= model_data_dir+\"/json/\"\n",
        "!mkdir JSON_DIR\n",
        "\n",
        "print(\"Model directory :\",MODEL_DIR)\n",
        "print(\"Json directory :\",JSON_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woB9Co_dtyUV"
      },
      "source": [
        "Code for extracting RAR items to base Colab directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2jRhRp8jXOq"
      },
      "source": [
        "%%time\n",
        "!apt install unrar\n",
        "\n",
        "cpCmd = [\"cp\",ds_rar_loc, \".\"]\n",
        "process = subprocess.Popen(cpCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "unrarCmd = [\"unrar\",\"x\",\"-y\",dataset_rar]\n",
        "process = subprocess.Popen(unrarCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "cpCmd = [\"cp\",model_rar_loc, \".\"]\n",
        "process = subprocess.Popen(cpCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "unrarCmd = [\"unrar\",\"x\",\"-y\",model_rar]\n",
        "process = subprocess.Popen(unrarCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "rmCmd = [\"rm\",dataset_rar]\n",
        "process = subprocess.Popen(rmCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "rmCmd = [\"rm\",model_rar]\n",
        "process = subprocess.Popen(unrarCmd, stdout=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0uvRoC4uRhu"
      },
      "source": [
        "### Code for running the model\n",
        "Modify enhance_data and transfer_with_full_training paramters as per your requirements\n",
        "\n",
        "For more details, refer to the [imageAI docs](https://imageai.readthedocs.io/en/latest/custom/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugIlUkJ1iNm"
      },
      "source": [
        "\n",
        "model.setDataDirectory(data_directory=DATASET_NAME, train_subdirectory=r\"train/\", test_subdirectory=r\"val/\",models_subdirectory=MODEL_DIR, json_subdirectory=JSON_DIR)\n",
        "model.trainModel(num_objects= num_classes, num_experiments=NUM_EPOCHS, enhance_data=False, batch_size=8, transfer_from_model=MODEL_H5, transfer_with_full_training=False, show_network_summary=True, save_full_model =True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T0qQqbMLPDd"
      },
      "source": [
        "'''\n",
        "%%time\n",
        "\n",
        "# RUN ONLY IF PREVIOUS PART FAILED\n",
        "last_saved_iter = r\"path of the last checkpointed model h5\"\n",
        "epochs_left = 20\n",
        "model.setDataDirectory(data_directory=DATASET_NAME train_subdirectory=r\"train/\", test_subdirectory=r\"val/\",models_subdirectory=MODEL_DIR, json_subdirectory=JSON_DIR)\n",
        "model.trainModel(num_objects= num_classes, num_experiments=epochs_left, enhance_data=False, batch_size=8,transfer_from_model= last_saved_iter, transfer_with_full_training=False, show_network_summary=True, save_full_model =True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q_d9OFTuuUq"
      },
      "source": [
        "### Sample Image prediction code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npRb-UsTVcuz"
      },
      "source": [
        "#Example - squeezemod = getModelPredict(\"SqueezeNet\",MODEL_DIR+\"/model_ex-041_acc-0.982818.h5\",JSON_DIR+\"/model_class.json\",10)\n",
        "squeezemod = getModelPredict(\"SqueezeNet\",MODEL_DIR+\"/model_to_be_tested\",JSON_DIR+\"/model_class.json\",num_classes)\n",
        "prediction.loadFullModel(num_objects=20)\n",
        "predictions, probabilities = prediction.predictImage(r\"image_path\", result_count=5)\n",
        "print(predictions,probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POFP8Y50uuEw"
      },
      "source": [
        "### Code for saving model to the TensorFlow SavedModel format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kpBCpEIZSjE"
      },
      "source": [
        "squeezemod = getModelPredict(\"SqueezeNet\",MODEL_DIR+\"/model_name\",JSON_DIR+\"/model_class.json\",num_classes)\n",
        "modelTransfer = squeezemod._CustomImagePrediction__model_collection[0]\n",
        "modelTransfer.save('/content/drive/MyDrive/saved/modelName')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}